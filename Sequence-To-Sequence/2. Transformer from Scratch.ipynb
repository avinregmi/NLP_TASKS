{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Transformer from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be implementing the famous Transformer architecture from scratch.\n",
    "\n",
    "The code is based off of the following repos/blog posts:\n",
    "\n",
    "- [attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch)\n",
    "- [pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) \n",
    "\n",
    "Thanks so much to their authors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the keys to understanding how any model works is understanding how the shapes of the tensors change during the processing of each part. We'll be using the logging module to output debugging information to help our understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"tensor_shapes\")\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        '%(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "# if you want the model to continuously print tensor shapes, set to DEBUG!\n",
    "logger.setLevel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def getclass():\n",
    "    stack = inspect.stack()\n",
    "    return stack[3][0].f_locals[\"self\"].__class__\n",
    "\n",
    "# A helper function to check how tensor sizes change\n",
    "def log_size(tsr: torch.Tensor, name: str):\n",
    "    cls = getclass()\n",
    "    logger.log(level=cls.level, msg=f\"[{cls.__name__}] {name} size={tsr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use logging levels to control the modules we receive output from. The lower the logging level, the more tensor information you'll get. Feel free to play around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "# Control how much debugging output we want\n",
    "class TensorLoggingLevels(IntEnum):\n",
    "    attention = 1\n",
    "    attention_head = 2\n",
    "    multihead_attention_block = 3\n",
    "    enc_dec_block = 4\n",
    "    enc_dec = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using an enum to refer to dimensions whenever possible to improve readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dim(IntEnum):\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer is an attention-based architecture. The attention used in the Transformer is the scaled dot product attention, represented by the following formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### torch.bmm: https://pytorch.org/docs/stable/torch.html#torch.bmm\n",
    "Performs a batch matrix-matrix product of matrices stored in input and mat2.\n",
    "\n",
    "*input and mat2 must be 3-D tensors each containing the same number of matrices ie. (batch).*\n",
    "\n",
    "\n",
    "If input is a(b×n×m) tensor, mat2 is a (b×m×p) tensor, out will be a (b×n×p) tensor.\n",
    "\n",
    "##### torch.transpose: https://pytorch.org/docs/stable/torch.html#torch.transpose\n",
    "torch.transpose(input, dim0, dim1) → Tensor\n",
    "\n",
    "Returns a tensor that is a transposed version of input. The given dimensions dim0 and dim1 are swapped.\n",
    "\n",
    "##### torch.sum https://pytorch.org/docs/stable/torch.html#torch.sum\n",
    "Returns the sum of each row of the input tensor in the given dimension dim. If dim is a list of dimensions, reduce over all of them.\n",
    "\n",
    "If keepdim is True, the output tensor is of the same size as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textrm{Attention}(Q, K, V) = \\textrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](./assets/scaled-dot-product-attention-steps.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.attention # Logging level: \n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q,k,v size = 5,10,20 (Batch, seq, feature)\n",
    "        \n",
    "        # Check if key and query size are the same\n",
    "        d_k = k.size(-1) # get the size of the key\n",
    "        assert q.size(-1) == d_k\n",
    "\n",
    "        # STEP 1: Calculate score\n",
    "        # compute the dot product between queries and keys for each batch and position in the sequence\n",
    "        k = k.transpose(Dim.seq, Dim.feature) #1, 2 #k.size: 5,20,10\n",
    "        score_attn = torch.bmm(q, k) # (Batch, Seq, Seq)\n",
    "        # we get an attention score between each position in the sequence for each batch\n",
    "\n",
    "        \n",
    "        # STEP 2: Divide by sqrt(Dk)\n",
    "        # scale the dot products by the dimensionality. Normalize the weights across the sequence dimension\n",
    "        score_attn = score_attn / math.sqrt(d_k)\n",
    "        # (Note that since we transposed, the sequence and feature dimensions are switched)\n",
    "        \n",
    "        # STEP 3: Mask Optional\n",
    "        # fill attention weights with 0s where padded\n",
    "        if mask is not None: score_attn = score_attn.masked_fill(mask, 0)\n",
    "\n",
    "        # STEP 4: Softmax    \n",
    "        score_attn = torch.exp(score_attn)\n",
    "        log_size(score_attn, \"attention weight\") # (Batch, Seq, Seq)\n",
    "        score_attn = score_attn / score_attn.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        score_attn = self.dropout(score_attn)\n",
    "        \n",
    "        \n",
    "        # STEP 5: Matmul with value matrix\n",
    "        output = torch.bmm(score_attn, v) # (Batch, Seq, Feature)\n",
    "        \n",
    "        log_size(output, \"attention output size\") # (Batch, Seq, Seq)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = ScaledDotProductAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(5, 10, 20)\n",
    "k = torch.rand(5, 10, 20)\n",
    "v = torch.rand(5, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7417, 0.6113, 0.5676, 0.5910, 0.5614, 0.6212, 0.5809, 0.5165,\n",
       "          0.4292, 0.5281, 0.3755, 0.7405, 0.7570, 0.6680, 0.5172, 0.6548,\n",
       "          0.5355, 0.5850, 0.5552, 0.4886],\n",
       "         [0.7447, 0.6208, 0.5685, 0.6119, 0.5555, 0.6326, 0.5775, 0.5204,\n",
       "          0.4490, 0.5376, 0.3877, 0.7308, 0.7370, 0.6580, 0.5193, 0.6560,\n",
       "          0.5501, 0.6061, 0.5813, 0.5215],\n",
       "         [0.6723, 0.5889, 0.4827, 0.5745, 0.4901, 0.5346, 0.5524, 0.4718,\n",
       "          0.3610, 0.4852, 0.3613, 0.6532, 0.6971, 0.6579, 0.4699, 0.6321,\n",
       "          0.5309, 0.5361, 0.4772, 0.4326],\n",
       "         [0.6854, 0.5194, 0.5535, 0.5398, 0.5483, 0.6098, 0.5069, 0.4946,\n",
       "          0.4210, 0.4893, 0.3324, 0.6358, 0.6551, 0.5674, 0.5028, 0.5502,\n",
       "          0.4830, 0.5634, 0.4816, 0.5012],\n",
       "         [0.7511, 0.6158, 0.5658, 0.5963, 0.5664, 0.6350, 0.5755, 0.5155,\n",
       "          0.4336, 0.5376, 0.3873, 0.7479, 0.7472, 0.6654, 0.5167, 0.6549,\n",
       "          0.5400, 0.5954, 0.5667, 0.5085],\n",
       "         [0.6320, 0.5101, 0.4774, 0.4922, 0.5057, 0.5145, 0.5229, 0.4019,\n",
       "          0.4421, 0.4068, 0.3727, 0.6508, 0.7181, 0.5713, 0.5095, 0.5498,\n",
       "          0.4725, 0.5527, 0.5181, 0.4517],\n",
       "         [0.6836, 0.5761, 0.5096, 0.4991, 0.5310, 0.5804, 0.5061, 0.4222,\n",
       "          0.4045, 0.4858, 0.3566, 0.7355, 0.7164, 0.6031, 0.4523, 0.5844,\n",
       "          0.4557, 0.5498, 0.5237, 0.4430],\n",
       "         [0.5784, 0.3953, 0.2872, 0.4200, 0.3440, 0.4414, 0.4181, 0.4104,\n",
       "          0.1935, 0.3655, 0.2468, 0.5161, 0.4725, 0.4715, 0.3447, 0.5420,\n",
       "          0.3944, 0.3530, 0.3583, 0.2752],\n",
       "         [0.6638, 0.5999, 0.5512, 0.5593, 0.5065, 0.5494, 0.5706, 0.4700,\n",
       "          0.3738, 0.4846, 0.3166, 0.6340, 0.6881, 0.6169, 0.5103, 0.5594,\n",
       "          0.4959, 0.5088, 0.4840, 0.4869],\n",
       "         [0.7564, 0.6073, 0.5678, 0.5966, 0.5603, 0.6396, 0.5896, 0.5218,\n",
       "          0.4592, 0.5189, 0.3717, 0.7183, 0.7357, 0.6345, 0.5499, 0.6391,\n",
       "          0.5393, 0.6051, 0.5815, 0.5350]],\n",
       "\n",
       "        [[0.5114, 0.5917, 0.5279, 0.4780, 0.6168, 0.4773, 0.6112, 0.4779,\n",
       "          0.6672, 0.5816, 0.5732, 0.4148, 0.4617, 0.4739, 0.8139, 0.5437,\n",
       "          0.5203, 0.5422, 0.4481, 0.6762],\n",
       "         [0.3816, 0.5165, 0.4369, 0.3344, 0.5132, 0.3557, 0.4155, 0.3646,\n",
       "          0.5484, 0.4374, 0.5167, 0.3314, 0.3670, 0.3978, 0.6776, 0.4882,\n",
       "          0.3454, 0.4312, 0.3739, 0.5500],\n",
       "         [0.4885, 0.5221, 0.4804, 0.4623, 0.6027, 0.4230, 0.6032, 0.4414,\n",
       "          0.6471, 0.5637, 0.4787, 0.3949, 0.4668, 0.4843, 0.7070, 0.5423,\n",
       "          0.4226, 0.5573, 0.3906, 0.5737],\n",
       "         [0.5248, 0.6058, 0.5480, 0.4971, 0.6141, 0.4777, 0.6107, 0.4688,\n",
       "          0.6833, 0.5955, 0.5511, 0.4060, 0.4791, 0.4870, 0.8154, 0.5425,\n",
       "          0.5192, 0.5520, 0.4381, 0.6611],\n",
       "         [0.4639, 0.5227, 0.4731, 0.4120, 0.5322, 0.4010, 0.4984, 0.4466,\n",
       "          0.6182, 0.5068, 0.4136, 0.2993, 0.4418, 0.3670, 0.6861, 0.4228,\n",
       "          0.4119, 0.4784, 0.3282, 0.5791],\n",
       "         [0.4160, 0.4660, 0.4343, 0.4459, 0.5047, 0.4079, 0.4843, 0.4747,\n",
       "          0.5956, 0.5144, 0.4395, 0.3189, 0.4146, 0.3821, 0.6494, 0.4581,\n",
       "          0.4351, 0.4150, 0.3951, 0.5748],\n",
       "         [0.4164, 0.5367, 0.4741, 0.3558, 0.5546, 0.4135, 0.4858, 0.3446,\n",
       "          0.5385, 0.4972, 0.5461, 0.3643, 0.3749, 0.4395, 0.7240, 0.4992,\n",
       "          0.3984, 0.4666, 0.4153, 0.5961],\n",
       "         [0.5187, 0.5947, 0.5333, 0.4670, 0.6261, 0.4813, 0.6211, 0.4709,\n",
       "          0.6597, 0.5834, 0.5702, 0.4142, 0.4618, 0.4730, 0.8187, 0.5382,\n",
       "          0.5150, 0.5554, 0.4450, 0.6824],\n",
       "         [0.4527, 0.5539, 0.4849, 0.4546, 0.5397, 0.4153, 0.5156, 0.4725,\n",
       "          0.6431, 0.5100, 0.5137, 0.3566, 0.4335, 0.4164, 0.7497, 0.4985,\n",
       "          0.4617, 0.4740, 0.3997, 0.6128],\n",
       "         [0.5089, 0.5545, 0.5135, 0.4228, 0.5884, 0.4569, 0.5787, 0.4358,\n",
       "          0.6168, 0.5616, 0.4540, 0.3388, 0.4540, 0.4077, 0.7441, 0.4402,\n",
       "          0.4629, 0.5321, 0.3667, 0.6359]],\n",
       "\n",
       "        [[0.5214, 0.5434, 0.5816, 0.2960, 0.3462, 0.5973, 0.4783, 0.4175,\n",
       "          0.5848, 0.5073, 0.6043, 0.3428, 0.5302, 0.4255, 0.3940, 0.4130,\n",
       "          0.3174, 0.5562, 0.4095, 0.3429],\n",
       "         [0.5860, 0.5849, 0.6690, 0.2976, 0.4139, 0.6705, 0.5560, 0.4238,\n",
       "          0.6377, 0.5791, 0.6402, 0.4016, 0.6003, 0.4179, 0.3961, 0.4305,\n",
       "          0.3374, 0.5456, 0.4601, 0.3887],\n",
       "         [0.5856, 0.5844, 0.6703, 0.2934, 0.4115, 0.6673, 0.5672, 0.4241,\n",
       "          0.6286, 0.6027, 0.6199, 0.3966, 0.6011, 0.4365, 0.4002, 0.4317,\n",
       "          0.3242, 0.5386, 0.4618, 0.3834],\n",
       "         [0.4773, 0.5455, 0.5553, 0.2010, 0.3751, 0.5730, 0.4328, 0.3741,\n",
       "          0.5475, 0.5071, 0.6153, 0.3209, 0.5912, 0.3373, 0.3982, 0.3747,\n",
       "          0.2681, 0.4338, 0.3320, 0.2964],\n",
       "         [0.5229, 0.5639, 0.5885, 0.2963, 0.3907, 0.6176, 0.5084, 0.4210,\n",
       "          0.5820, 0.5354, 0.5764, 0.3439, 0.5163, 0.3952, 0.3801, 0.3802,\n",
       "          0.2952, 0.5016, 0.4440, 0.3813],\n",
       "         [0.4749, 0.5398, 0.6358, 0.2944, 0.3752, 0.5564, 0.4939, 0.3849,\n",
       "          0.5341, 0.5591, 0.5225, 0.3858, 0.5480, 0.4368, 0.4055, 0.3399,\n",
       "          0.3063, 0.5191, 0.4347, 0.3643],\n",
       "         [0.4061, 0.4512, 0.4858, 0.1771, 0.3486, 0.3833, 0.3414, 0.3085,\n",
       "          0.3732, 0.3654, 0.4959, 0.2148, 0.4177, 0.1915, 0.2817, 0.2453,\n",
       "          0.1903, 0.2641, 0.2874, 0.2090],\n",
       "         [0.5817, 0.5829, 0.6707, 0.3010, 0.4057, 0.6667, 0.5622, 0.4239,\n",
       "          0.6283, 0.5991, 0.6182, 0.3939, 0.5970, 0.4447, 0.4065, 0.4233,\n",
       "          0.3320, 0.5560, 0.4645, 0.3899],\n",
       "         [0.5853, 0.5836, 0.6816, 0.3119, 0.4115, 0.6659, 0.5789, 0.4274,\n",
       "          0.6347, 0.5872, 0.6135, 0.4131, 0.5895, 0.4368, 0.3958, 0.4210,\n",
       "          0.3445, 0.5623, 0.4792, 0.4025],\n",
       "         [0.5864, 0.5885, 0.6844, 0.3102, 0.4285, 0.6419, 0.5651, 0.4216,\n",
       "          0.6374, 0.5740, 0.6321, 0.3961, 0.5836, 0.4296, 0.4083, 0.4160,\n",
       "          0.3314, 0.5440, 0.4679, 0.3750]],\n",
       "\n",
       "        [[0.5849, 0.5364, 0.6118, 0.5672, 0.4723, 0.5655, 0.5795, 0.6250,\n",
       "          0.6165, 0.5878, 0.6212, 0.5653, 0.6874, 0.6405, 0.5315, 0.6105,\n",
       "          0.5112, 0.6645, 0.4841, 0.5890],\n",
       "         [0.5835, 0.5089, 0.5657, 0.5128, 0.4136, 0.4929, 0.5367, 0.6067,\n",
       "          0.5108, 0.5275, 0.5862, 0.5383, 0.6643, 0.6015, 0.5333, 0.4972,\n",
       "          0.5129, 0.6258, 0.4592, 0.5414],\n",
       "         [0.4447, 0.4176, 0.5006, 0.5419, 0.3326, 0.5357, 0.5160, 0.5894,\n",
       "          0.6022, 0.5669, 0.6183, 0.5405, 0.6487, 0.5291, 0.4786, 0.5658,\n",
       "          0.4333, 0.5487, 0.3929, 0.4909],\n",
       "         [0.4665, 0.4451, 0.5616, 0.3971, 0.4057, 0.4804, 0.4532, 0.5213,\n",
       "          0.4734, 0.4607, 0.5667, 0.4747, 0.5744, 0.5308, 0.5053, 0.4612,\n",
       "          0.4805, 0.5557, 0.4204, 0.4590],\n",
       "         [0.5840, 0.5175, 0.5820, 0.5708, 0.4615, 0.4932, 0.5396, 0.5793,\n",
       "          0.5996, 0.5135, 0.5371, 0.4872, 0.6403, 0.5609, 0.4776, 0.5432,\n",
       "          0.4859, 0.6066, 0.4338, 0.5197],\n",
       "         [0.6021, 0.5596, 0.6174, 0.5627, 0.4690, 0.5608, 0.6038, 0.6330,\n",
       "          0.6195, 0.5865, 0.6082, 0.5584, 0.6843, 0.6488, 0.5054, 0.6054,\n",
       "          0.5082, 0.6840, 0.5048, 0.6094],\n",
       "         [0.2990, 0.2529, 0.2956, 0.3832, 0.3114, 0.3521, 0.3328, 0.3921,\n",
       "          0.3773, 0.4175, 0.4326, 0.3977, 0.4594, 0.3839, 0.3315, 0.4795,\n",
       "          0.2337, 0.3594, 0.2206, 0.3699],\n",
       "         [0.5677, 0.5243, 0.5956, 0.5562, 0.4546, 0.5664, 0.5912, 0.6278,\n",
       "          0.6228, 0.6061, 0.6246, 0.5791, 0.6827, 0.6393, 0.4983, 0.6351,\n",
       "          0.4875, 0.6655, 0.4885, 0.6074],\n",
       "         [0.5844, 0.5320, 0.6024, 0.5908, 0.4770, 0.5608, 0.5747, 0.6275,\n",
       "          0.6186, 0.5846, 0.6228, 0.5581, 0.6992, 0.6417, 0.5378, 0.6013,\n",
       "          0.5138, 0.6545, 0.4689, 0.5809],\n",
       "         [0.5246, 0.5203, 0.5083, 0.4876, 0.3541, 0.5086, 0.5678, 0.5831,\n",
       "          0.6077, 0.5574, 0.5276, 0.5207, 0.5843, 0.5670, 0.4349, 0.5974,\n",
       "          0.4030, 0.6162, 0.4857, 0.5908]],\n",
       "\n",
       "        [[0.6738, 0.6015, 0.4456, 0.6013, 0.4652, 0.3787, 0.6214, 0.5185,\n",
       "          0.3685, 0.5475, 0.7969, 0.6356, 0.4704, 0.2774, 0.5256, 0.5900,\n",
       "          0.6797, 0.4672, 0.6309, 0.5509],\n",
       "         [0.6385, 0.5327, 0.4222, 0.5421, 0.4335, 0.3625, 0.5706, 0.4781,\n",
       "          0.3720, 0.4919, 0.7202, 0.5952, 0.4481, 0.1932, 0.4769, 0.5546,\n",
       "          0.6549, 0.4111, 0.5452, 0.5241],\n",
       "         [0.5618, 0.5244, 0.4216, 0.5367, 0.4050, 0.3194, 0.5464, 0.4456,\n",
       "          0.2669, 0.5181, 0.6686, 0.6104, 0.4225, 0.2700, 0.4757, 0.5688,\n",
       "          0.5609, 0.3961, 0.6013, 0.4974],\n",
       "         [0.7027, 0.6154, 0.4656, 0.6045, 0.4775, 0.3843, 0.6091, 0.5352,\n",
       "          0.3778, 0.5288, 0.8138, 0.6150, 0.4670, 0.2955, 0.5203, 0.5675,\n",
       "          0.6845, 0.4867, 0.6046, 0.5615],\n",
       "         [0.6025, 0.4445, 0.3829, 0.4203, 0.4008, 0.2739, 0.4383, 0.4552,\n",
       "          0.3299, 0.3606, 0.6396, 0.4128, 0.3484, 0.2511, 0.3152, 0.3481,\n",
       "          0.5644, 0.4573, 0.4154, 0.3810],\n",
       "         [0.6407, 0.5465, 0.4412, 0.4824, 0.4199, 0.3759, 0.5048, 0.5055,\n",
       "          0.3490, 0.4814, 0.7509, 0.5449, 0.4083, 0.2603, 0.4942, 0.5091,\n",
       "          0.5970, 0.4494, 0.5128, 0.5436],\n",
       "         [0.6157, 0.4984, 0.3621, 0.5026, 0.4667, 0.3657, 0.5470, 0.4701,\n",
       "          0.3347, 0.4769, 0.7136, 0.5977, 0.4423, 0.2438, 0.4645, 0.5798,\n",
       "          0.5991, 0.4189, 0.5990, 0.4766],\n",
       "         [0.7010, 0.5991, 0.4659, 0.5796, 0.4638, 0.3802, 0.6150, 0.5383,\n",
       "          0.3972, 0.5266, 0.8151, 0.6108, 0.4683, 0.2706, 0.5223, 0.5640,\n",
       "          0.6893, 0.4818, 0.5947, 0.5656],\n",
       "         [0.6933, 0.6127, 0.4562, 0.5978, 0.4660, 0.3810, 0.6165, 0.5323,\n",
       "          0.3885, 0.5326, 0.8112, 0.6113, 0.4645, 0.2838, 0.5254, 0.5675,\n",
       "          0.6943, 0.4728, 0.6009, 0.5648],\n",
       "         [0.6400, 0.5580, 0.4419, 0.5498, 0.4201, 0.3535, 0.5731, 0.4905,\n",
       "          0.3821, 0.5019, 0.7245, 0.5910, 0.4440, 0.2040, 0.4867, 0.5405,\n",
       "          0.6631, 0.4032, 0.5354, 0.5261]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(q, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we turn to the core component in the Transformer architecture: the multi-head attention block. This block applies linear transformations to the input, then applies scaled dot product attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i2.wp.com/mlexplained.com/wp-content/uploads/2017/12/multi_head_attention.png?zoom=2&resize=224%2C293)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    level = TensorLoggingLevels.attention_head\n",
    "    def __init__(self, d_model, d_feature, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # We will assume the queries, keys, and values all have the same feature size\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        self.query_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.key_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.value_tfm = nn.Linear(d_model, d_feature)\n",
    "\n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        Q = self.query_tfm(queries) # (Batch, Seq, Feature)\n",
    "        K = self.key_tfm(keys) # (Batch, Seq, Feature)\n",
    "        V = self.value_tfm(values) # (Batch, Seq, Feature)\n",
    "        log_size(Q, \"queries, keys, vals\")\n",
    "        # compute multiple attention weighted sums\n",
    "        x = self.attn(Q, K, V)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.5142e-01, -1.5114e-01,  1.8654e-01, -1.5269e-02, -2.0554e-02,\n",
       "          -2.6997e-01,  3.6803e-01,  3.2500e-01, -4.4870e-01,  2.9316e-01,\n",
       "           6.6881e-01, -4.7138e-01,  1.1629e-02,  4.9833e-02, -1.0520e-01,\n",
       "           3.2230e-01, -8.6525e-02,  2.3126e-01,  9.3712e-03,  4.5779e-01],\n",
       "         [ 2.0075e-01, -9.7678e-02,  8.3727e-02,  4.0386e-02, -6.9687e-02,\n",
       "          -2.2826e-01,  3.1940e-01,  2.7730e-01, -3.1985e-01,  1.6904e-01,\n",
       "           4.8410e-01, -3.7480e-01, -1.6219e-02,  5.6603e-02, -1.2206e-01,\n",
       "           2.7324e-01, -4.3632e-02,  1.8233e-01, -4.7860e-03,  3.5455e-01],\n",
       "         [ 2.2820e-01, -1.5924e-01,  1.9143e-01,  3.2332e-03,  3.3688e-02,\n",
       "          -2.6777e-01,  3.0001e-01,  3.0079e-01, -3.2704e-01,  2.6293e-01,\n",
       "           6.1354e-01, -3.5711e-01,  1.8396e-02,  8.2449e-02, -1.1521e-01,\n",
       "           2.9854e-01, -6.6571e-02,  1.9163e-01,  1.8653e-02,  3.7795e-01],\n",
       "         [ 2.8846e-01, -2.0000e-01,  1.8258e-01, -6.9057e-03, -1.8119e-02,\n",
       "          -3.1183e-01,  3.5457e-01,  3.0683e-01, -3.8106e-01,  3.3247e-01,\n",
       "           6.5723e-01, -4.4744e-01,  1.8692e-02,  5.7667e-02, -1.4475e-01,\n",
       "           3.5604e-01, -8.2650e-02,  2.4896e-01, -2.4099e-02,  4.4344e-01],\n",
       "         [ 2.6230e-01, -1.8893e-01,  1.5412e-01, -1.9602e-03, -2.1820e-02,\n",
       "          -3.1882e-01,  3.9661e-01,  3.2050e-01, -3.7506e-01,  2.5851e-01,\n",
       "           6.4176e-01, -4.5687e-01, -2.1918e-02,  5.3995e-02, -1.5415e-01,\n",
       "           3.4913e-01, -8.4638e-02,  2.3253e-01,  3.2931e-03,  4.3716e-01],\n",
       "         [ 2.8885e-01, -2.0136e-01,  1.9840e-01, -1.2505e-02, -1.4451e-02,\n",
       "          -3.3512e-01,  4.2228e-01,  3.5737e-01, -4.3546e-01,  3.3059e-01,\n",
       "           7.3129e-01, -4.9040e-01,  6.0373e-03,  6.0183e-02, -1.4474e-01,\n",
       "           3.8772e-01, -9.8734e-02,  2.6088e-01, -1.7441e-03,  4.9156e-01],\n",
       "         [ 2.4412e-01, -1.2885e-01,  1.7788e-01,  4.0374e-02, -2.7614e-02,\n",
       "          -2.9444e-01,  3.3803e-01,  3.1246e-01, -3.3241e-01,  2.7309e-01,\n",
       "           5.8648e-01, -4.0744e-01,  1.2179e-02,  6.5183e-02, -1.1389e-01,\n",
       "           3.3230e-01, -5.0737e-02,  1.9718e-01, -1.1716e-02,  4.0799e-01],\n",
       "         [ 2.8996e-01, -2.0065e-01,  2.0117e-01, -1.1285e-02, -1.2329e-02,\n",
       "          -3.3591e-01,  4.2094e-01,  3.5717e-01, -4.3480e-01,  3.3410e-01,\n",
       "           7.3255e-01, -4.9086e-01,  8.2798e-03,  6.1551e-02, -1.4451e-01,\n",
       "           3.8843e-01, -9.6488e-02,  2.6028e-01, -2.4893e-03,  4.9201e-01],\n",
       "         [ 2.9177e-01, -2.0036e-01,  2.0208e-01, -9.6617e-03, -1.6244e-02,\n",
       "          -3.3836e-01,  4.2006e-01,  3.5693e-01, -4.3390e-01,  3.3453e-01,\n",
       "           7.2964e-01, -4.9075e-01,  7.0622e-03,  5.9865e-02, -1.4380e-01,\n",
       "           3.9176e-01, -9.6099e-02,  2.6263e-01, -4.4805e-03,  4.9192e-01],\n",
       "         [ 2.6159e-01, -1.8309e-01,  1.5789e-01,  5.2464e-04, -2.3800e-02,\n",
       "          -3.1654e-01,  3.9093e-01,  3.2015e-01, -3.7826e-01,  2.5918e-01,\n",
       "           6.3860e-01, -4.5664e-01, -1.9380e-02,  5.4307e-02, -1.5040e-01,\n",
       "           3.4894e-01, -8.1055e-02,  2.3245e-01,  2.7481e-03,  4.3573e-01]],\n",
       "\n",
       "        [[ 2.5380e-01, -2.4245e-01,  1.0574e-01, -4.7321e-02, -2.9557e-02,\n",
       "          -2.4564e-01,  3.1260e-01,  4.3907e-01, -3.6564e-01,  2.0629e-01,\n",
       "           6.6999e-01, -3.4638e-01, -5.5314e-02,  2.1066e-01, -2.0341e-01,\n",
       "           4.4068e-01,  7.0573e-02,  3.5728e-01,  4.2624e-02,  3.5010e-01],\n",
       "         [ 2.2895e-01, -2.2168e-01,  1.4192e-01, -2.1497e-02, -1.5593e-02,\n",
       "          -2.6322e-01,  2.3036e-01,  4.4073e-01, -3.4429e-01,  1.9971e-01,\n",
       "           6.6230e-01, -3.3783e-01, -2.7516e-02,  1.9332e-01, -1.6633e-01,\n",
       "           3.6477e-01,  7.1965e-02,  3.2385e-01,  5.8427e-02,  3.2375e-01],\n",
       "         [ 2.2590e-01, -2.0431e-01,  1.4177e-01, -7.9398e-03, -3.6284e-02,\n",
       "          -2.4624e-01,  2.2955e-01,  4.4481e-01, -3.2859e-01,  1.5450e-01,\n",
       "           6.3301e-01, -3.4950e-01, -2.6868e-02,  2.1021e-01, -1.5538e-01,\n",
       "           3.3832e-01,  7.3922e-02,  2.6548e-01,  8.3822e-02,  2.9415e-01],\n",
       "         [ 3.0047e-01, -2.8383e-01,  1.3837e-01, -2.6172e-02, -2.1703e-02,\n",
       "          -2.9122e-01,  3.1499e-01,  5.0871e-01, -3.9590e-01,  2.2756e-01,\n",
       "           7.7119e-01, -3.9135e-01, -6.5829e-02,  2.5937e-01, -2.3176e-01,\n",
       "           4.7445e-01,  9.1817e-02,  4.0051e-01,  4.8435e-02,  4.0594e-01],\n",
       "         [ 2.9698e-01, -2.8371e-01,  1.3730e-01, -2.9661e-02, -2.4428e-02,\n",
       "          -2.9285e-01,  3.1834e-01,  5.0830e-01, -3.9520e-01,  2.2760e-01,\n",
       "           7.6949e-01, -3.9136e-01, -6.3988e-02,  2.5609e-01, -2.3578e-01,\n",
       "           4.7427e-01,  8.8813e-02,  3.9883e-01,  4.8435e-02,  4.0177e-01],\n",
       "         [ 2.9466e-01, -2.8394e-01,  1.3687e-01, -3.0163e-02, -2.7232e-02,\n",
       "          -2.9444e-01,  3.2047e-01,  5.1170e-01, -3.9797e-01,  2.2545e-01,\n",
       "           7.7134e-01, -3.9399e-01, -6.4999e-02,  2.5576e-01, -2.3543e-01,\n",
       "           4.7237e-01,  8.4292e-02,  3.9800e-01,  4.9171e-02,  4.0660e-01],\n",
       "         [ 2.7563e-01, -2.6120e-01,  1.3465e-01, -9.2671e-03,  1.6987e-02,\n",
       "          -2.4545e-01,  2.8753e-01,  4.4784e-01, -3.5947e-01,  1.9193e-01,\n",
       "           6.9496e-01, -3.4616e-01, -7.3554e-02,  2.3750e-01, -1.9653e-01,\n",
       "           4.2615e-01,  9.5733e-02,  3.7003e-01,  2.9960e-02,  3.9623e-01],\n",
       "         [ 2.9680e-01, -2.8351e-01,  1.3722e-01, -2.8549e-02, -2.5516e-02,\n",
       "          -2.9335e-01,  3.1717e-01,  5.1142e-01, -3.9703e-01,  2.2667e-01,\n",
       "           7.7218e-01, -3.9282e-01, -6.4377e-02,  2.5735e-01, -2.3449e-01,\n",
       "           4.7357e-01,  8.8175e-02,  3.9913e-01,  4.9766e-02,  4.0352e-01],\n",
       "         [ 2.9702e-01, -2.8361e-01,  1.3696e-01, -2.8618e-02, -2.6178e-02,\n",
       "          -2.9405e-01,  3.1678e-01,  5.1232e-01, -3.9698e-01,  2.2771e-01,\n",
       "           7.7265e-01, -3.9334e-01, -6.4447e-02,  2.5733e-01, -2.3557e-01,\n",
       "           4.7436e-01,  8.9047e-02,  4.0057e-01,  4.8707e-02,  4.0339e-01],\n",
       "         [ 2.3588e-01, -2.5038e-01,  1.3079e-01, -4.6370e-02,  1.8193e-03,\n",
       "          -2.5841e-01,  2.7357e-01,  3.6103e-01, -2.8720e-01,  2.0218e-01,\n",
       "           5.9690e-01, -2.9012e-01, -3.8133e-02,  1.8175e-01, -2.2270e-01,\n",
       "           3.7094e-01,  4.7264e-02,  3.0611e-01,  2.9412e-02,  3.1386e-01]],\n",
       "\n",
       "        [[ 2.2884e-01, -1.9921e-01,  2.5151e-01, -5.4123e-02, -5.8309e-02,\n",
       "          -3.5928e-01,  2.7436e-01,  3.9709e-01, -4.2634e-01,  3.6079e-01,\n",
       "           6.3226e-01, -4.8989e-01,  1.2319e-03,  5.1523e-02, -2.1137e-01,\n",
       "           3.7952e-01,  2.6402e-04,  3.1716e-01, -2.4396e-02,  4.3727e-01],\n",
       "         [ 2.4587e-01, -2.4137e-01,  2.7113e-01, -7.3774e-02, -2.8194e-02,\n",
       "          -3.9439e-01,  2.8202e-01,  4.4858e-01, -4.6580e-01,  3.9274e-01,\n",
       "           7.2729e-01, -5.1088e-01,  8.3865e-03,  7.1277e-02, -2.0606e-01,\n",
       "           4.3668e-01, -5.8882e-03,  3.5717e-01,  2.7107e-03,  4.6006e-01],\n",
       "         [ 2.0143e-01, -2.0213e-01,  2.2202e-01, -6.7694e-02, -2.1988e-02,\n",
       "          -3.3805e-01,  2.4247e-01,  3.5058e-01, -3.7911e-01,  3.0218e-01,\n",
       "           5.9534e-01, -4.5953e-01,  1.1510e-02,  3.0655e-02, -1.5098e-01,\n",
       "           3.2429e-01, -2.1401e-02,  2.7189e-01,  2.4901e-02,  3.7477e-01],\n",
       "         [ 2.4604e-01, -2.3766e-01,  2.6913e-01, -7.1570e-02, -2.9092e-02,\n",
       "          -3.9253e-01,  2.7987e-01,  4.4916e-01, -4.6665e-01,  3.9132e-01,\n",
       "           7.2575e-01, -5.0894e-01,  6.1415e-03,  7.3220e-02, -2.0568e-01,\n",
       "           4.3724e-01, -4.5456e-03,  3.5761e-01,  1.4240e-03,  4.6238e-01],\n",
       "         [ 2.2781e-01, -2.0173e-01,  2.1292e-01, -5.5416e-02, -2.7241e-02,\n",
       "          -3.5777e-01,  2.4119e-01,  4.0853e-01, -4.2611e-01,  3.3301e-01,\n",
       "           6.5659e-01, -4.7297e-01, -3.2665e-03,  6.8526e-02, -1.6363e-01,\n",
       "           3.9287e-01, -8.9232e-03,  3.1846e-01,  1.3822e-02,  4.3186e-01],\n",
       "         [ 2.4578e-01, -2.4086e-01,  2.7326e-01, -7.4736e-02, -2.6747e-02,\n",
       "          -3.9346e-01,  2.8363e-01,  4.4509e-01, -4.6677e-01,  3.9407e-01,\n",
       "           7.2470e-01, -5.1163e-01,  6.7254e-03,  7.0015e-02, -2.0547e-01,\n",
       "           4.3718e-01, -6.3315e-03,  3.5672e-01,  8.8608e-04,  4.6116e-01],\n",
       "         [ 2.4552e-01, -2.3784e-01,  2.6946e-01, -7.1205e-02, -2.9647e-02,\n",
       "          -3.9372e-01,  2.8057e-01,  4.5105e-01, -4.6686e-01,  3.9293e-01,\n",
       "           7.2724e-01, -5.0825e-01,  6.3410e-03,  7.3390e-02, -2.0488e-01,\n",
       "           4.3952e-01, -4.1733e-03,  3.5904e-01,  2.2510e-03,  4.6224e-01],\n",
       "         [ 1.9472e-01, -1.8269e-01,  1.8791e-01, -5.7183e-02, -4.2462e-02,\n",
       "          -2.9621e-01,  2.1831e-01,  3.5837e-01, -3.6763e-01,  2.8735e-01,\n",
       "           5.5606e-01, -3.9156e-01,  1.8405e-02,  6.5371e-02, -1.8664e-01,\n",
       "           3.2817e-01, -1.1730e-04,  2.7753e-01, -1.3615e-02,  3.6876e-01],\n",
       "         [ 2.4537e-01, -2.3858e-01,  2.7373e-01, -7.3362e-02, -2.5353e-02,\n",
       "          -3.9134e-01,  2.8090e-01,  4.4620e-01, -4.6738e-01,  3.9299e-01,\n",
       "           7.2569e-01, -5.0802e-01,  4.8905e-03,  7.2540e-02, -2.0545e-01,\n",
       "           4.3734e-01, -4.9225e-03,  3.5769e-01,  7.6947e-04,  4.6193e-01],\n",
       "         [ 2.4625e-01, -2.3843e-01,  2.6941e-01, -7.2956e-02, -2.7454e-02,\n",
       "          -3.9150e-01,  2.7913e-01,  4.4678e-01, -4.6611e-01,  3.9031e-01,\n",
       "           7.2453e-01, -5.0958e-01,  6.0424e-03,  7.2690e-02, -2.0581e-01,\n",
       "           4.3566e-01, -5.3704e-03,  3.5573e-01,  1.2510e-03,  4.6085e-01]],\n",
       "\n",
       "        [[ 3.5392e-01, -1.5991e-01,  2.6738e-01,  9.1727e-02, -7.3110e-02,\n",
       "          -3.9916e-01,  3.0706e-01,  3.9979e-01, -3.5593e-01,  3.8358e-01,\n",
       "           7.0159e-01, -4.4721e-01,  7.5448e-02,  9.8720e-02, -5.5248e-02,\n",
       "           4.3791e-01,  2.8011e-02,  2.8297e-01, -2.1542e-03,  4.3088e-01],\n",
       "         [ 3.4228e-01, -1.3200e-01,  3.2774e-01,  8.2543e-02, -8.4693e-02,\n",
       "          -3.8272e-01,  3.3137e-01,  3.8818e-01, -4.1806e-01,  4.3954e-01,\n",
       "           6.8619e-01, -4.9286e-01,  6.4441e-02,  1.0096e-01, -7.5451e-02,\n",
       "           4.5357e-01,  6.6526e-02,  3.1398e-01, -3.8530e-02,  4.7078e-01],\n",
       "         [ 3.7573e-01, -1.7286e-01,  3.3650e-01,  7.3442e-02, -7.8313e-02,\n",
       "          -4.2756e-01,  3.6914e-01,  4.1947e-01, -4.1859e-01,  4.6175e-01,\n",
       "           7.5287e-01, -5.2317e-01,  8.7701e-02,  9.7537e-02, -7.2840e-02,\n",
       "           4.9053e-01,  4.6019e-02,  3.1124e-01, -2.4179e-02,  4.7272e-01],\n",
       "         [ 3.4211e-01, -1.3106e-01,  3.2719e-01,  8.1056e-02, -8.8478e-02,\n",
       "          -3.8440e-01,  3.3337e-01,  3.8846e-01, -4.1896e-01,  4.4060e-01,\n",
       "           6.8666e-01, -4.9429e-01,  6.4523e-02,  1.0079e-01, -7.8018e-02,\n",
       "           4.5436e-01,  6.3333e-02,  3.1273e-01, -3.7758e-02,  4.7113e-01],\n",
       "         [ 3.7564e-01, -1.6970e-01,  3.3607e-01,  7.5468e-02, -8.4605e-02,\n",
       "          -4.2997e-01,  3.7023e-01,  4.2257e-01, -4.2116e-01,  4.6091e-01,\n",
       "           7.5543e-01, -5.2534e-01,  8.5687e-02,  9.9700e-02, -7.6296e-02,\n",
       "           4.8940e-01,  4.1558e-02,  3.1157e-01, -2.3687e-02,  4.7820e-01],\n",
       "         [ 2.6131e-01, -1.2938e-01,  2.7513e-01,  3.3946e-02, -4.0839e-02,\n",
       "          -3.2695e-01,  3.1283e-01,  3.2954e-01, -3.3970e-01,  3.1896e-01,\n",
       "           6.2295e-01, -3.8469e-01,  6.0638e-02,  5.1523e-02, -5.0894e-02,\n",
       "           3.4918e-01, -4.7165e-02,  2.1868e-01, -1.3920e-02,  3.6898e-01],\n",
       "         [ 3.5136e-01, -1.5764e-01,  2.6842e-01,  9.0911e-02, -7.3506e-02,\n",
       "          -3.9819e-01,  3.0670e-01,  3.9851e-01, -3.5656e-01,  3.8182e-01,\n",
       "           7.0019e-01, -4.4491e-01,  7.5394e-02,  9.7163e-02, -5.6356e-02,\n",
       "           4.3538e-01,  2.6490e-02,  2.8235e-01, -1.9783e-03,  4.2983e-01],\n",
       "         [ 2.8027e-01, -1.4889e-01,  1.9701e-01,  3.9164e-02, -8.4086e-02,\n",
       "          -3.4996e-01,  2.7887e-01,  3.4749e-01, -3.0927e-01,  3.1330e-01,\n",
       "           5.8665e-01, -3.7653e-01,  5.8325e-02,  6.4650e-02, -7.9867e-02,\n",
       "           3.8809e-01,  5.5890e-03,  2.5163e-01,  8.7878e-03,  3.4337e-01],\n",
       "         [ 3.7491e-01, -1.7166e-01,  3.3656e-01,  7.2632e-02, -8.3325e-02,\n",
       "          -4.2979e-01,  3.7158e-01,  4.2063e-01, -4.2006e-01,  4.6258e-01,\n",
       "           7.5298e-01, -5.2648e-01,  8.6740e-02,  9.8624e-02, -7.7233e-02,\n",
       "           4.9013e-01,  4.3134e-02,  3.1059e-01, -2.3815e-02,  4.7500e-01],\n",
       "         [ 2.5008e-01, -1.3399e-01,  2.1708e-01, -8.5349e-03, -1.1081e-01,\n",
       "          -3.2513e-01,  3.0970e-01,  3.1773e-01, -3.4410e-01,  3.5052e-01,\n",
       "           5.4761e-01, -4.0110e-01,  5.5066e-02,  4.6980e-02, -1.0381e-01,\n",
       "           3.9450e-01, -6.5605e-03,  2.4317e-01, -1.6540e-02,  3.3246e-01]],\n",
       "\n",
       "        [[ 3.4420e-01, -1.7275e-01,  1.3342e-01,  6.8575e-02, -6.7829e-02,\n",
       "          -3.4209e-01,  3.8094e-01,  3.3654e-01, -2.6497e-01,  2.9093e-01,\n",
       "           6.1340e-01, -3.6693e-01, -6.1496e-02,  9.1029e-02, -1.8263e-01,\n",
       "           5.1038e-01, -2.2261e-02,  2.5056e-01,  1.7036e-02,  3.8799e-01],\n",
       "         [ 3.1086e-01, -1.7047e-01,  1.0729e-01,  2.6139e-02, -2.5793e-02,\n",
       "          -3.1181e-01,  3.4599e-01,  2.6843e-01, -2.3264e-01,  2.4224e-01,\n",
       "           5.1752e-01, -3.1444e-01, -8.7805e-02,  9.4294e-02, -1.3255e-01,\n",
       "           4.7274e-01, -1.8884e-02,  2.1725e-01,  2.8291e-02,  3.6218e-01],\n",
       "         [ 2.5917e-01, -1.3253e-01,  1.2703e-01,  4.6934e-02,  1.1232e-02,\n",
       "          -2.5445e-01,  2.7898e-01,  2.1351e-01, -1.8444e-01,  2.2858e-01,\n",
       "           4.1331e-01, -2.4964e-01, -7.7189e-02,  5.1843e-02, -5.8979e-02,\n",
       "           3.9194e-01,  2.8583e-02,  2.0119e-01, -9.3730e-03,  3.2940e-01],\n",
       "         [ 3.2181e-01, -1.5797e-01,  1.0630e-01,  6.3163e-02, -6.2509e-02,\n",
       "          -3.0958e-01,  3.2823e-01,  2.9969e-01, -2.2432e-01,  2.4087e-01,\n",
       "           5.5876e-01, -3.2370e-01, -7.0563e-02,  8.9510e-02, -1.6855e-01,\n",
       "           4.4481e-01, -3.6464e-02,  2.2017e-01,  3.9354e-02,  3.5130e-01],\n",
       "         [ 3.0286e-01, -1.6897e-01,  9.3623e-02,  5.2689e-02, -6.9547e-03,\n",
       "          -2.7008e-01,  2.8445e-01,  2.4467e-01, -1.8626e-01,  2.1044e-01,\n",
       "           4.7981e-01, -2.8056e-01, -6.6008e-02,  1.2508e-01, -1.1448e-01,\n",
       "           4.3525e-01,  1.7613e-03,  1.8566e-01,  2.9618e-02,  2.9008e-01],\n",
       "         [ 3.1260e-01, -1.5798e-01,  8.7907e-02,  8.1581e-02, -4.0671e-02,\n",
       "          -2.6563e-01,  2.6581e-01,  2.7167e-01, -1.7969e-01,  2.0192e-01,\n",
       "           5.1778e-01, -2.8752e-01, -5.2143e-02,  1.2254e-01, -1.4925e-01,\n",
       "           4.0521e-01, -1.9719e-02,  1.8618e-01,  4.5348e-02,  2.7933e-01],\n",
       "         [ 2.7369e-01, -1.2822e-01,  1.4250e-01,  7.3424e-02,  4.1532e-03,\n",
       "          -2.5334e-01,  3.0333e-01,  2.6908e-01, -2.1900e-01,  2.5521e-01,\n",
       "           5.0050e-01, -2.6382e-01, -1.9478e-02,  7.2172e-02, -9.3865e-02,\n",
       "           4.1289e-01,  2.5169e-02,  2.1590e-01, -1.4981e-02,  3.3398e-01],\n",
       "         [ 3.4611e-01, -1.7305e-01,  1.3551e-01,  7.2269e-02, -6.4737e-02,\n",
       "          -3.4417e-01,  3.8211e-01,  3.3669e-01, -2.6301e-01,  2.9266e-01,\n",
       "           6.1593e-01, -3.6797e-01, -6.1556e-02,  9.0724e-02, -1.7980e-01,\n",
       "           5.1306e-01, -2.0358e-02,  2.5061e-01,  1.7327e-02,  3.8773e-01],\n",
       "         [ 3.4564e-01, -1.7304e-01,  1.3212e-01,  7.1393e-02, -6.7779e-02,\n",
       "          -3.4372e-01,  3.8114e-01,  3.3720e-01, -2.6274e-01,  2.9041e-01,\n",
       "           6.1559e-01, -3.6872e-01, -6.1884e-02,  9.2721e-02, -1.8318e-01,\n",
       "           5.1281e-01, -2.2795e-02,  2.4874e-01,  1.8942e-02,  3.8451e-01],\n",
       "         [ 3.4705e-01, -1.7286e-01,  1.3517e-01,  7.3237e-02, -6.6415e-02,\n",
       "          -3.4592e-01,  3.8314e-01,  3.3614e-01, -2.6138e-01,  2.9312e-01,\n",
       "           6.1476e-01, -3.6842e-01, -6.3368e-02,  8.9301e-02, -1.8092e-01,\n",
       "           5.1419e-01, -2.0741e-02,  2.5119e-01,  1.6763e-02,  3.8858e-01]]],\n",
       "       grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_head = AttentionHead(20, 20)\n",
    "attn_head(q, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-head attention block simply applies multiple attention heads, then concatenates the outputs and applies a single linear projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll supress logging from the scaled dot product attention now\n",
    "logger.setLevel(TensorLoggingLevels.attention_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### repeat() : Repeats this tensor along the specified dimensions. https://pytorch.org/docs/stable/tensors.html#torch.Tensor.repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.multihead_attention_block\n",
    "    def __init__(self, d_model, d_feature, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_feature = d_feature\n",
    "        self.n_heads = n_heads\n",
    "        # in practice, d_model == d_feature * n_heads\n",
    "        assert d_model == d_feature * n_heads\n",
    "\n",
    "        # Note that this is very inefficient:\n",
    "        # I am merely implementing the heads separately because it is \n",
    "        # easier to understand this way\n",
    "        self.attn_heads = nn.ModuleList([\n",
    "            AttentionHead(d_model, d_feature, dropout) for _ in range(n_heads)\n",
    "        ])\n",
    "        self.projection = nn.Linear(d_feature * n_heads, d_model) \n",
    "    \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        log_size(queries, \"Input queries\")\n",
    "        x = [attn(queries, keys, values, mask=mask) # (Batch, Seq, Feature)\n",
    "             for i, attn in enumerate(self.attn_heads)]\n",
    "        log_size(x[0], \"output of single head\")\n",
    "        \n",
    "        # reconcatenate\n",
    "        x = torch.cat(x, dim=Dim.feature) # (Batch, Seq, D_Feature * n_heads)\n",
    "        log_size(x, \"concatenated output\")\n",
    "        \n",
    "        # Final linear operation\n",
    "        x = self.projection(x) # (Batch, Seq, D_Model)\n",
    "        log_size(x, \"projected output\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 160])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 160])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 160])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 20])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 20])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 20])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 20])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 160])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 160])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 160])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 160])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 160])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 160])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1593, -0.1515,  0.1382,  ..., -0.2638, -0.1695,  0.1944],\n",
       "         [-0.1466, -0.1222,  0.1361,  ..., -0.1853, -0.1153,  0.1861],\n",
       "         [-0.1503, -0.1859,  0.0968,  ..., -0.2396, -0.1392,  0.2225],\n",
       "         ...,\n",
       "         [-0.1632, -0.1102,  0.0614,  ..., -0.2435, -0.1446,  0.1956],\n",
       "         [-0.1915, -0.1331,  0.1185,  ..., -0.2642, -0.1337,  0.1914],\n",
       "         [-0.1897, -0.1741,  0.0944,  ..., -0.2772, -0.1739,  0.1774]],\n",
       "\n",
       "        [[-0.1200, -0.1194,  0.1329,  ..., -0.1804, -0.0830,  0.1663],\n",
       "         [-0.1086, -0.1202,  0.1108,  ..., -0.1850, -0.0800,  0.1741],\n",
       "         [-0.1288, -0.1390,  0.1101,  ..., -0.1954, -0.1033,  0.1758],\n",
       "         ...,\n",
       "         [-0.1141, -0.0744,  0.0339,  ..., -0.2044, -0.1073,  0.1878],\n",
       "         [-0.0914, -0.1333,  0.1424,  ..., -0.1461, -0.0915,  0.1575],\n",
       "         [-0.1207, -0.1414,  0.1184,  ..., -0.1859, -0.0994,  0.1658]],\n",
       "\n",
       "        [[-0.1660, -0.1548,  0.1685,  ..., -0.1551, -0.0853,  0.1674],\n",
       "         [-0.1831, -0.1423,  0.1554,  ..., -0.1675, -0.0867,  0.1678],\n",
       "         [-0.1621, -0.1764,  0.1653,  ..., -0.1893, -0.0929,  0.1803],\n",
       "         ...,\n",
       "         [-0.1543, -0.1791,  0.1711,  ..., -0.1736, -0.1176,  0.1851],\n",
       "         [-0.1307, -0.1551,  0.1446,  ..., -0.1588, -0.0990,  0.1702],\n",
       "         [-0.1490, -0.1890,  0.1975,  ..., -0.1538, -0.0758,  0.1359]],\n",
       "\n",
       "        [[-0.1674, -0.1718,  0.1821,  ..., -0.2348, -0.0996,  0.1656],\n",
       "         [-0.1863, -0.1387,  0.1695,  ..., -0.2404, -0.1240,  0.1308],\n",
       "         [-0.1776, -0.1438,  0.1099,  ..., -0.2248, -0.1411,  0.1542],\n",
       "         ...,\n",
       "         [-0.1489, -0.1649,  0.1960,  ..., -0.2041, -0.0799,  0.1411],\n",
       "         [-0.1718, -0.1301,  0.1683,  ..., -0.2282, -0.1207,  0.1168],\n",
       "         [-0.1929, -0.1120,  0.0978,  ..., -0.2514, -0.0870,  0.1607]],\n",
       "\n",
       "        [[-0.1545, -0.0776,  0.1048,  ..., -0.2786, -0.2089,  0.1473],\n",
       "         [-0.1820, -0.0208,  0.0709,  ..., -0.3269, -0.1784,  0.1588],\n",
       "         [-0.1596, -0.0707,  0.1234,  ..., -0.2843, -0.1858,  0.1355],\n",
       "         ...,\n",
       "         [-0.1566, -0.0300,  0.0366,  ..., -0.3075, -0.2157,  0.1793],\n",
       "         [-0.1905, -0.0336,  0.0204,  ..., -0.3076, -0.1795,  0.1753],\n",
       "         [-0.1555, -0.0585,  0.1342,  ..., -0.2915, -0.1880,  0.1441]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#d_model & d_feature are dimension for our linear operation. Ie. nn.Linear(d_model, d_feature)\n",
    "d_model = 20 * 8\n",
    "d_feature = 20\n",
    "n_heads = 8\n",
    "\n",
    "heads = MultiHeadAttention(d_model, d_feature, n_heads)\n",
    "heads(q.repeat(1, 1, 8), \n",
    "      k.repeat(1, 1, 8), \n",
    "      v.repeat(1, 1, 8))\n",
    "\n",
    "#q.repeat(1,1,8).size() => torch.Size([5, 10, 160])\n",
    "#k.repeat(1,1,8).size() => torch.Size([5, 10, 160])\n",
    "#v.repeat(1,1,8).size() => torch.Size([5, 10, 160])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these core components in place, implementing the encoder is pretty easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./assets/encoder-steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder consists of the following components:\n",
    "- A multi-head attention block\n",
    "- A simple feedforward neural network\n",
    "\n",
    "These components are connected using residual connections and layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll supress logging from the individual attention heads\n",
    "logger.setLevel(TensorLoggingLevels.multihead_attention_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer normalization is similar to batch normalization, but normalizes across the feature dimension instead of the batch dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i1.wp.com/mlexplained.com/wp-content/uploads/2018/01/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2018-01-11-11.48.12.png?w=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-8):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder just stacks these together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec_block\n",
    "    def __init__(self, d_model=512, d_feature=64,\n",
    "                 d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        \n",
    "        self.layer_norm1 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.position_wise_feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        \n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        log_size(x, \"Encoder block input\")\n",
    "        \n",
    "        # STEP 1\n",
    "        att = self.attn_head(x, x, x, mask=mask)\n",
    "        log_size(x, \"Attention output\")\n",
    "        \n",
    "        # STEP 2\n",
    "        # Apply normalization and residual connection\n",
    "        x = x + self.dropout(self.layer_norm1(att))\n",
    "        \n",
    "        # STEP 3\n",
    "        # Apply position-wise feedforward network\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        log_size(x, \"Feedforward output\")\n",
    "        \n",
    "        # STEP 4\n",
    "        # Apply normalization and residual connection\n",
    "        x = x + self.dropout(self.layer_norm2(pos))\n",
    "        log_size(x, \"Encoder size output\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = EncoderBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EncoderBlock] Encoder block input size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 512])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[AttentionHead] queries, keys, vals size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention weight size=torch.Size([5, 10, 10])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[ScaledDotProductAttention] attention output size size=torch.Size([5, 10, 64])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 64])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 64])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 10, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0108,  1.0342,  0.5324,  ..., -0.9076,  1.1966, -1.9913],\n",
       "         [ 1.9889,  1.0661, -0.2731,  ...,  0.1506,  0.7549, -1.8243],\n",
       "         [-0.1332,  1.1098, -0.8688,  ..., -0.7683,  1.2096, -2.3394],\n",
       "         ...,\n",
       "         [ 1.3884,  1.6255,  0.4349,  ..., -0.1239,  1.5403, -1.1731],\n",
       "         [ 0.1003,  1.7683, -0.3073,  ..., -0.3618,  1.2750,  0.2557],\n",
       "         [-0.1606,  1.5961,  0.3885,  ..., -0.0957,  0.9493, -2.0467]],\n",
       "\n",
       "        [[-0.0468,  2.0994,  0.4432,  ..., -0.0439,  1.3845, -2.3946],\n",
       "         [ 0.9454,  1.8005,  0.5436,  ...,  0.8469,  0.5925, -1.7992],\n",
       "         [ 0.2430,  1.5768,  0.2729,  ..., -0.8551,  1.6647, -1.8886],\n",
       "         ...,\n",
       "         [-0.5455,  1.7989, -0.4916,  ...,  1.5479, -0.0375, -1.1887],\n",
       "         [ 0.8584,  3.1980,  0.5049,  ..., -0.3918,  0.5090, -1.8666],\n",
       "         [ 0.2035,  2.3547, -0.6481,  ..., -0.1720,  1.6655, -1.4895]],\n",
       "\n",
       "        [[ 0.3635,  1.3213,  0.4483,  ..., -0.5096,  1.0134, -1.8842],\n",
       "         [-0.8481,  0.9727,  0.2459,  ..., -0.9888,  2.6016, -0.5819],\n",
       "         [ 0.0171,  1.3222,  0.5457,  ..., -1.0750,  0.5188, -2.1856],\n",
       "         ...,\n",
       "         [ 0.1189,  1.7751,  0.5387,  ..., -0.2373,  1.1607, -1.9659],\n",
       "         [-0.4943,  0.5103,  0.5833,  ..., -0.2956,  2.3002, -1.0777],\n",
       "         [-0.3292,  1.6022,  0.2450,  ..., -0.3587,  1.1248, -1.4169]],\n",
       "\n",
       "        [[ 1.4657,  0.9358, -0.2227,  ...,  0.3385,  1.5336, -1.4126],\n",
       "         [ 1.5301,  2.3467, -0.3068,  ...,  0.1518,  1.1612, -2.7229],\n",
       "         [ 0.9965,  1.5840,  0.7273,  ..., -0.4465,  1.7521, -1.0460],\n",
       "         ...,\n",
       "         [ 0.3695,  1.4234,  0.7744,  ..., -0.3774,  1.4882, -1.8033],\n",
       "         [ 0.4700,  1.9475, -0.0800,  ...,  1.0067,  1.1263, -0.7160],\n",
       "         [ 1.2284,  2.2906, -0.1901,  ..., -1.3235,  0.8983, -0.2749]],\n",
       "\n",
       "        [[ 0.0921,  1.4249,  0.5421,  ..., -0.4854,  0.9484, -1.1027],\n",
       "         [ 1.1741,  2.1518, -0.2722,  ..., -0.8582,  0.5386, -0.6542],\n",
       "         [ 0.0330,  1.3221, -0.4317,  ..., -0.2779,  1.1819, -1.5182],\n",
       "         ...,\n",
       "         [-0.2406,  2.2723,  0.0324,  ...,  0.0257,  0.8040, -0.4622],\n",
       "         [-0.4170,  0.4989, -0.5652,  ..., -0.2783,  1.3366, -0.7258],\n",
       "         [ 0.2061,  2.3899, -1.1410,  ..., -0.7538,  1.1723, -1.7501]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc(torch.rand(5, 10, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder consists of 6 consecutive encoder blocks, so can simply be implemented like the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec\n",
    "    def __init__(self, n_blocks=6, d_model=512,\n",
    "                 n_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderBlock(d_model=d_model, d_feature=d_model // n_heads,\n",
    "                         d_ff=d_ff, dropout=dropout)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.FloatTensor, mask=None):\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is mostly the same as the encoder. There's just one additional multi-head attention block that takes the target sentence as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./assets/decoder-steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keys and values are the outputs of the encoder, and the queries are the outputs of the multi-head attention over the target entence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec_block\n",
    "    def __init__(self, d_model=512, d_feature=64,\n",
    "                 d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        \n",
    "        self.attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        \n",
    "        self.position_wise_feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(d_model)\n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        self.layer_norm3 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_out, \n",
    "                src_mask=None, tgt_mask=None):\n",
    "        # Step 1\n",
    "        # Apply attention to inputs\n",
    "        att = self.masked_attn_head(x, x, x, mask=src_mask)\n",
    "        x = x + self.dropout(self.layer_norm1(att))\n",
    "        \n",
    "        # Step 2 \n",
    "        # Apply attention to the encoder outputs and outputs of the previous layer\n",
    "        att = self.attn_head(queries=x, keys=enc_out, values=enc_out, mask=tgt_mask)\n",
    "        x = x + self.dropout(self.layer_norm2(att))\n",
    "        \n",
    "        # Step 3\n",
    "        # Apply position-wise feedforward network\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        x = x + self.dropout(self.layer_norm2(pos))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EncoderBlock] Encoder block input size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 10, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 10, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 10, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 10, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0089, -0.1621, -1.7238,  ..., -0.2977, -0.0777,  0.6444],\n",
       "         [-0.6819,  1.0120, -2.2252,  ..., -0.7179, -0.4535,  0.4894],\n",
       "         [-0.2763, -1.2217, -0.4251,  ..., -1.1588,  0.5523,  0.2806],\n",
       "         ...,\n",
       "         [-0.9637,  0.0112, -1.8797,  ..., -1.4102, -0.3725,  1.9287],\n",
       "         [-1.0610,  0.0212, -0.1304,  ..., -0.9032, -0.1420, -0.0251],\n",
       "         [ 0.0933,  0.3426, -0.6407,  ..., -0.3894,  0.3314,  1.4376]],\n",
       "\n",
       "        [[-0.5598,  0.2414, -1.2082,  ..., -1.3604,  0.7519,  0.3092],\n",
       "         [-0.1707, -0.4689, -2.7463,  ..., -1.5195, -0.6086,  0.0534],\n",
       "         [-0.0418,  0.4068, -1.9493,  ..., -1.8899, -0.0944,  0.9726],\n",
       "         ...,\n",
       "         [ 0.0187, -0.0970, -1.2934,  ..., -1.6659, -0.3797, -1.0419],\n",
       "         [-0.6441, -1.6213, -2.8262,  ..., -1.1339,  0.4981,  0.1986],\n",
       "         [-0.3408,  0.8055, -2.1190,  ..., -1.1125, -0.3273,  0.6981]],\n",
       "\n",
       "        [[ 0.4155,  2.4197, -1.8789,  ..., -1.0938,  1.2254,  1.3786],\n",
       "         [-1.2558,  0.5798, -0.9764,  ..., -0.8672,  0.7679,  0.9769],\n",
       "         [-0.9021,  0.4323, -1.6557,  ..., -1.8080, -0.3660, -0.0808],\n",
       "         ...,\n",
       "         [-2.1520,  0.4512, -1.7574,  ..., -0.6466,  0.0705,  0.7371],\n",
       "         [-0.4173,  0.4273, -1.5912,  ..., -0.4868,  0.1253, -0.5882],\n",
       "         [-0.3649,  1.1303, -0.8293,  ...,  0.5895, -0.4134,  0.4474]],\n",
       "\n",
       "        [[-1.2475,  1.0031, -1.0799,  ..., -1.4806,  0.9474,  0.1882],\n",
       "         [-1.8045, -0.4216, -2.1246,  ...,  0.6689,  0.2749, -0.5877],\n",
       "         [-0.0726,  0.9166,  0.7023,  ..., -1.1043,  0.6651, -0.7177],\n",
       "         ...,\n",
       "         [-1.9992,  0.4710, -1.0245,  ..., -1.1025,  1.4831,  0.6453],\n",
       "         [ 0.7588, -0.7872, -0.8642,  ..., -0.0543,  0.8443,  0.0426],\n",
       "         [-1.9741, -0.4873, -1.4063,  ..., -0.0772,  0.1567, -0.1907]],\n",
       "\n",
       "        [[-0.3777,  0.7430, -1.7491,  ..., -1.2825,  1.5669,  1.1665],\n",
       "         [-0.0810, -0.9685, -1.5420,  ..., -1.2902, -0.0933,  1.4904],\n",
       "         [-1.2597, -0.0263, -1.4361,  ..., -1.2398, -0.1851,  1.0668],\n",
       "         ...,\n",
       "         [ 0.4860,  0.9831, -0.9146,  ..., -1.6619,  0.6771,  0.5528],\n",
       "         [-0.4915,  3.0481, -1.8297,  ..., -1.2948,  0.8026, -0.3305],\n",
       "         [-0.9698,  1.3960, -2.7325,  ..., -0.7311, -0.7895, -0.0788]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = DecoderBlock()\n",
    "dec(torch.rand(5, 10, 512), enc(torch.rand(5, 10, 512)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the decoder is just a stack of the underlying block so is simple to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec\n",
    "    def __init__(self, n_blocks=6, d_model=512, d_feature=64,\n",
    "                 d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        self.decoders = nn.ModuleList([\n",
    "            DecoderBlock(d_model=d_model, d_feature=d_model // n_heads,\n",
    "                         d_ff=d_ff, dropout=dropout)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x: torch.FloatTensor, \n",
    "                enc_out: torch.FloatTensor, \n",
    "                src_mask=None, tgt_mask=None):\n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x, enc_out, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention blocks are just simple matrix multiplications: therefore they don't have any notion of order! The Transformer explicitly adds positional information via the positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    level = 1\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.weight = nn.Parameter(pe, requires_grad=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.weight[:, :x.size(1), :] # (1, Seq, Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPositionEmbedding(nn.Module):\n",
    "    level = 1\n",
    "    def __init__(self, vocab_size, d_model=512):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.LongTensor, mask=None) -> torch.FloatTensor:\n",
    "        return self.word_embedding(x) + self.position_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = WordPositionEmbedding(1000)\n",
    "encoder = TransformerEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 30, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 30, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 30, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 30, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 30, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] Input queries size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] output of single head size=torch.Size([5, 30, 64])\n",
      "[MultiHeadAttention] concatenated output size=torch.Size([5, 30, 512])\n",
      "[MultiHeadAttention] projected output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.1055, -0.8502, -4.9186,  ..., -1.2713, -4.2459,  0.2078],\n",
       "         [ 2.6354, -1.2489, -1.0282,  ...,  3.3799, -1.5298,  5.0705],\n",
       "         [-0.4808, -4.2213,  1.0662,  ...,  1.8674, -3.0999,  3.9541],\n",
       "         ...,\n",
       "         [-1.8011, -0.2372, -2.2997,  ...,  0.0451, -3.3087, 10.8904],\n",
       "         [ 3.9191, -1.6762, -3.7252,  ...,  5.9903, -0.6455,  3.0996],\n",
       "         [ 0.4039, -1.9685, -2.4440,  ..., -0.8608, -0.1490,  6.7031]],\n",
       "\n",
       "        [[ 1.5821,  3.1465,  0.7618,  ..., -0.3407, -0.1425,  3.2112],\n",
       "         [ 1.6676, -1.2685,  2.1302,  ...,  1.1090, -3.4212,  3.5214],\n",
       "         [ 3.1787,  2.5393,  3.9663,  ..., -3.6904, -2.7751,  2.0892],\n",
       "         ...,\n",
       "         [ 2.4655, -0.7829,  0.0596,  ..., -2.2599,  0.9266,  6.1969],\n",
       "         [ 0.4926,  2.0922,  3.7247,  ..., -3.8212, -0.7574,  2.4279],\n",
       "         [ 1.6447,  2.8645, -0.9687,  ..., -1.5437, -0.4660,  4.8466]],\n",
       "\n",
       "        [[ 0.4819,  4.5232,  1.0103,  ..., -8.8720, -3.7049,  3.7693],\n",
       "         [-0.3196,  2.4813,  5.1187,  ..., -6.5510,  2.7301,  3.8789],\n",
       "         [ 0.7460,  4.6172,  1.6897,  ..., -3.1239, -0.0925,  7.8727],\n",
       "         ...,\n",
       "         [ 2.1870,  2.5789,  4.0155,  ...,  4.1032,  1.1400,  3.9428],\n",
       "         [ 1.8951,  3.8942,  3.8287,  ...,  1.3588, -4.1338,  7.1291],\n",
       "         [-2.2998,  3.3236, -0.3151,  ..., -2.9119, -1.5182,  6.2831]],\n",
       "\n",
       "        [[ 0.9112,  3.3767,  5.2401,  ..., -1.8215, -3.2362,  4.5592],\n",
       "         [ 0.2526,  0.8315, -0.8525,  ..., -1.8621, -0.5996,  9.2102],\n",
       "         [ 0.3150, -1.7187,  4.4539,  ..., -1.0660, -0.8382,  5.1917],\n",
       "         ...,\n",
       "         [-3.7807,  0.6861,  0.2602,  ..., -3.1825,  2.1655,  7.1190],\n",
       "         [ 2.1806,  0.7563,  1.3264,  ..., -5.3251, -4.4565,  5.6285],\n",
       "         [-2.2253, -0.9348, -2.6068,  ..., -1.3316,  0.2412,  4.2150]],\n",
       "\n",
       "        [[-0.0823,  4.3263, -7.3579,  ..., -3.2837,  4.0665,  7.3243],\n",
       "         [-3.1840,  3.7760, -1.7565,  ..., -6.2594, -2.1000,  4.3961],\n",
       "         [ 1.6845,  0.9549, -1.4777,  ..., -3.2992,  2.0412,  3.8630],\n",
       "         ...,\n",
       "         [ 4.2335,  1.0664,  1.1905,  ..., -5.0066, -4.8280,  0.2452],\n",
       "         [-2.3642,  3.2592, -3.2682,  ..., -2.6533, -3.0306,  4.4230],\n",
       "         [ 1.0475, -0.9105, -2.5577,  ...,  0.7833, -4.7655,  2.1455]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(emb(torch.randint(1000, (5, 30))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put everything together now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://camo.githubusercontent.com/88e8f36ce61dedfd2491885b8df2f68c4d1f92f5/687474703a2f2f696d6775722e636f6d2f316b72463252362e706e67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll supress logging from the scaled dot product attention now\n",
    "logger.setLevel(TensorLoggingLevels.enc_dec_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = WordPositionEmbedding(1000)\n",
    "encoder = TransformerEncoder()\n",
    "decoder = TransformerDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder block input size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Attention output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Feedforward output size=torch.Size([5, 30, 512])\n",
      "[EncoderBlock] Encoder size output size=torch.Size([5, 30, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[  2.2477,   3.3096,  -2.8686,  ...,  -3.2644,  -2.4669,   6.1730],\n",
       "         [ -1.8250,  -4.9711,  -2.4306,  ...,  -2.9770,  -3.0167,   1.3833],\n",
       "         [  3.5309,   2.0060,  -3.2075,  ...,   0.1359,  -4.5663,  -1.5324],\n",
       "         ...,\n",
       "         [  7.0745,  -1.9728,   1.4077,  ...,   1.3778,  -0.3159,   1.5097],\n",
       "         [  5.8074,  -4.9769,   1.1956,  ...,  -0.4884,   0.6054,   2.2616],\n",
       "         [ -0.5769,   3.2016,  -0.5948,  ...,  -1.7086,  -3.5718,   3.0980]],\n",
       "\n",
       "        [[  4.1633,   1.9668,  -0.4448,  ...,  -0.5528,  -7.7785,  -0.5628],\n",
       "         [  6.3603,   4.5621,   1.4731,  ...,   2.1059,  -9.3620,  -4.9059],\n",
       "         [  1.9835,  -2.7217,  -1.0474,  ...,   1.4569,  -7.1355,   0.1433],\n",
       "         ...,\n",
       "         [  2.3934,   1.2819,   1.9249,  ...,  -3.7213,  -9.2985,  -5.8557],\n",
       "         [ -0.0274,   0.3156,  -0.6774,  ...,   1.5031, -10.2572,  -4.0842],\n",
       "         [ -2.2030,   4.4538,   0.1307,  ...,  -1.5143, -10.7648,  -1.3035]],\n",
       "\n",
       "        [[  3.4523,   8.5233,  -1.9825,  ...,  -0.4623,  -5.3684,  -3.9416],\n",
       "         [ -0.7156,   6.2328,   3.3172,  ...,   1.8109,  -6.4316,  -1.7234],\n",
       "         [  4.8919,   2.3916,   1.0952,  ...,  -3.4979,  -2.5825,  -0.9506],\n",
       "         ...,\n",
       "         [  6.4903,   3.9892,   1.4304,  ...,  -2.0206,  -8.6142,  -2.1427],\n",
       "         [  6.6442,   0.4645,   4.0409,  ...,  -0.1903,  -8.8341,  -1.8669],\n",
       "         [  3.1665,   2.5837,   0.3495,  ...,   2.3616,  -8.0547,  -2.9726]],\n",
       "\n",
       "        [[ -1.2516,   2.5482,   3.7852,  ...,  -2.7302,  -8.2792,  -0.5623],\n",
       "         [ -1.9989,   5.0016,   1.2062,  ...,  -0.4174,  -7.5535,   0.1456],\n",
       "         [  0.1048,  -2.4558,   4.1504,  ...,   5.7925, -10.0637,  -3.2967],\n",
       "         ...,\n",
       "         [  1.9609,   0.1642,   3.4059,  ...,   1.4830,  -6.8213,   3.5435],\n",
       "         [  0.0261,  -1.5271,   7.1073,  ...,   0.1010,  -8.1517,  -0.7287],\n",
       "         [  2.0211,  -5.9433,   3.4916,  ...,  -1.2228,  -6.8002,  -4.6091]],\n",
       "\n",
       "        [[  5.7406,  -2.8757,   0.5068,  ...,  -1.8448,  -1.3602,  -2.9466],\n",
       "         [ 10.9692,  -0.8585,  -0.9014,  ...,  -6.6807,  -1.1723,   5.8687],\n",
       "         [  4.0704,  -1.5752,  -0.3160,  ...,  -7.8781,  -4.8607,   5.1882],\n",
       "         ...,\n",
       "         [  5.8090,   0.4198,   6.7087,  ...,  -0.1035, -10.4693,   4.4274],\n",
       "         [  1.5856,  -3.4986,   2.7267,  ...,   0.0600,  -8.2686,   1.5979],\n",
       "         [  1.1402,  -2.8793,   5.7706,  ...,  -2.1236,  -6.9868,  -1.2790]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_ids = torch.randint(1000, (5, 30))\n",
    "tgt_ids = torch.randint(1000, (5, 30))\n",
    "x = encoder(emb(src_ids))\n",
    "decoder(emb(tgt_ids), x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "842px",
    "left": "1097px",
    "right": "20px",
    "top": "279px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
