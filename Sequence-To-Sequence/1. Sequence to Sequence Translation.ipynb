{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "We will be translating from French to English. This task is an example of sequence to sequence (seq2seq). Seq2seq can be more challenging than classification, since the output is of variable length (and typically different from the length of the input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True,\n",
    "            batch_first= True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True,\n",
    "            batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download and load the train, validation and test data. \n",
    "\n",
    "The dataset we'll be using is the [Multi30k dataset](https://github.com/multi30k/dataset). This is a dataset with ~30,000 parallel English, German and French sentences, each with ~12 words per sentence. \n",
    "\n",
    "`exts` specifies which languages to use as the source and target (source goes first) and `fields` specifies which field to use for the source and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 7855\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<unk>', '<pad>', '<sos>')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRG.vocab.itos[0],TRG.vocab.itos[1],TRG.vocab.itos[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,    4, 2550,  ...,    1,    1,    1],\n",
      "        [   2,    4,  123,  ...,    1,    1,    1],\n",
      "        [   2,    4,    0,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   2,    4,    0,  ...,    1,    1,    1],\n",
      "        [   2,    4,  338,  ...,    1,    1,    1],\n",
      "        [   2,    4,  713,  ...,    1,    1,    1]], device='cuda:0')\n",
      "tensor([[  2,   9, 136,  ...,   1,   1,   1],\n",
      "        [  2,   4,  26,  ...,   1,   1,   1],\n",
      "        [  2,   4,  53,  ...,   1,   1,   1],\n",
      "        ...,\n",
      "        [  2,   4,  70,  ...,   1,   1,   1],\n",
      "        [  2,  19, 119,  ...,   1,   1,   1],\n",
      "        [  2,   4,  64,  ...,   1,   1,   1]], device='cuda:0')\n",
      "batch.src.size:  torch.Size([64, 25])\n",
      "batch.trg.size:  torch.Size([64, 29])\n"
     ]
    }
   ],
   "source": [
    "# for i, batch in enumerate(train_iterator):\n",
    "#     src = batch.src\n",
    "#     trg = batch.trg\n",
    "\n",
    "batch = next(iter(train_iterator))\n",
    "print(batch.src)\n",
    "print(batch.trg)\n",
    "print(\"batch.src.size: \",batch.src.size())\n",
    "print(\"batch.trg.size: \",batch.trg.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Model\n",
    "\n",
    "\n",
    "### Encoders & Decoders\n",
    "The model in itself consists in an encoder and a decoder\n",
    "\n",
    "![Seq2seq model](assets/seq2seq.png)\n",
    "**Note: We are going from German to French**\n",
    "<center><i>Diagram from Smerity's <a href=\"https://smerity.com/articles/2016/google_nmt_arch.html\">Peeking into the neural network architecture used for Google's Neural Machine Translation</a></i></center>\n",
    "\n",
    "The encoder is a recurrent neural net and we feed it our input sentence, producing an output (that we discard for now) and a hidden state.  A **hidden state** is the activations that come out of an RNN.\n",
    "\n",
    "That hidden state is then given to the decoder (an other RNN) which uses it in conjunction with the outputs it predicts to get produce the translation. We loop until the decoder produces a padding token (or at 30 iterations to make sure it's not an infinite loop at the beginning of training). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRNN(nn.Module):\n",
    "    def __init__(self, src_vocab_size, target_vocab_size, \n",
    "                    hidden_size, output_seq_len, \n",
    "                    num_layers=2, bos_idx=0, pad_idx=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        emb_enc = nn.Embedding(src_vocab_size,300)\n",
    "        emb_dec = nn.Embedding(target_vocab_size,300)\n",
    "        \n",
    "        self.num_layers,self.hidden_size,self.output_seq_len = num_layers,hidden_size,output_seq_len\n",
    "        self.bos_idx,self.pad_idx = bos_idx,pad_idx #0,1\n",
    "        self.em_sz_enc = emb_enc.embedding_dim #300\n",
    "        self.em_sz_dec = emb_dec.embedding_dim #300\n",
    "        self.voc_sz_dec = emb_dec.num_embeddings #5893 Vocabulary Size for decoder\n",
    "                 \n",
    "        #Encoder\n",
    "        self.emb_enc = emb_enc\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.gru_enc = nn.GRU(input_size=self.em_sz_enc, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                              dropout=0.25, batch_first=True)\n",
    "        self.out_enc = nn.Linear(hidden_size, self.em_sz_dec, bias=False) #256, 300\n",
    "       \n",
    "        #Decoder\n",
    "        self.emb_dec = emb_dec\n",
    "        self.gru_dec = nn.GRU(input_size=self.em_sz_dec, hidden_size=self.em_sz_dec, num_layers=num_layers,\n",
    "                              dropout=0.1, batch_first=True)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out_dec = nn.Linear(self.em_sz_dec, self.voc_sz_dec) #300, 8144\n",
    "        \n",
    "        self.out_dec.weight.data = self.emb_dec.weight.data\n",
    "\n",
    "    def encoder(self, bs, inp): #bs:64 , inp.size: 64,x\n",
    "        # Dimension of embedding is 300\n",
    "        # h = (num_layers (2)*num_directions (1), batch  (64), hidden_size (256))\n",
    "        h = self.initHidden(bs) # h.size: 2,64,256\n",
    "        emb = self.emb_enc(inp) #emb.size: 64,x,300\n",
    "        emb = self.emb_enc_drop(emb) #emb.size: 64,x,300\n",
    "        _, h = self.gru_enc(emb, h) #h.size: 2,64,256\n",
    "        \n",
    "        # h(2,64,245) *  out_enc(256,300) = 2,64,300\n",
    "        h = self.out_enc(h) #h.size: 2,64,300\n",
    "        return h\n",
    "    \n",
    "    def decoder(self, dec_inp, h): #dec_inp.size: [64], h.size: 2,64,300 \n",
    "        emb = self.emb_dec(dec_inp).unsqueeze(1) #emb.size: 64, 1, 300 \n",
    "        outp, h = self.gru_dec(emb, h) #outp.size: [64, 1, 300], h.size: 2, 64, 300\n",
    "        outp = self.out_dec(self.out_drop(outp[:,0])) #outp.size: 64, 5893\n",
    "        return h, outp\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        bs, sl = inp.size()\n",
    "        #batch_size(bs) = 64\n",
    "        #seq_length(sl) = x variable\n",
    "        \n",
    "        h = self.encoder(bs, inp) #batch_size: 64, inp: x variable\n",
    "        #h.size: [2, 64, 300]\n",
    "        dec_inp = inp.new_zeros(bs).long() + self.bos_idx #[64] zeros\n",
    "        res = []\n",
    "        for i in range(self.output_seq_len): #self.output_seq_len: 30\n",
    "            h, outp = self.decoder(dec_inp, h) #dec_inp:64, #h = [batch_size: 64, inp: x variable]\n",
    "            # outp.size: [batch_size:64, emb_decoder:5893]\n",
    "            ## h.size: (num_layers (2)*num_directions (1), batch  (64), hidden_size (300))\n",
    "            #tensor.max(input, dim=0, keepdim=False, out=None):Returns the maximum value of all elements in the input tensor.\n",
    "            dec_inp = outp.max(1)[1]\n",
    "            res.append(outp)\n",
    "            '''\n",
    "            The all() function returns True if all items in an iterable(list, tuple, dictionary, etc.) are true, otherwise \n",
    "            it returns False.If the iterable object is empty, the all() function also returns True.\n",
    "            '''\n",
    "            if (dec_inp==self.pad_idx).all(): break #break if you detect pad_idx\n",
    "        return torch.stack(res, dim=1)\n",
    "    \n",
    "    def initHidden(self, bs): return one_param(self).new_zeros(self.num_layers, bs, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_param(m: nn.Module): \n",
    "    \"Return the first parameter of `m`.\"\n",
    "    return next(m.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqRNN(len(SRC.vocab), len(TRG.vocab), 256, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model(batch.src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,881,889 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqRNN(\n",
       "  (emb_enc): Embedding(7855, 300)\n",
       "  (emb_enc_drop): Dropout(p=0.15, inplace=False)\n",
       "  (gru_enc): GRU(300, 256, num_layers=2, batch_first=True, dropout=0.25)\n",
       "  (out_enc): Linear(in_features=256, out_features=300, bias=False)\n",
       "  (emb_dec): Embedding(5893, 300)\n",
       "  (gru_dec): GRU(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
       "  (out_drop): Dropout(p=0.35, inplace=False)\n",
       "  (out_dec): Linear(in_features=300, out_features=5893, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src)\n",
    "        \n",
    "        pad_idx=1\n",
    "        targ_len = trg.size()[1]\n",
    "\n",
    "        out_len = output.size()[1]\n",
    "        \n",
    "        '''\n",
    "        Our Taget and output has to be the same in sequence length. We have hard-coded input to be sequence length of 30.\n",
    "        If our target length is more than our output (targ_len>out_len): We need to add paading into our output length\n",
    "        If our target length is less than our output (out_len>targ_len): We need to add padding into our target length. \n",
    "        \n",
    "        '''\n",
    "        if targ_len>out_len:\n",
    "            output  = nn.functional.pad(output,  (0,0,0,targ_len-out_len,0,0), value=pad_idx)\n",
    "        if out_len>targ_len:\n",
    "            trg = nn.functional.pad(trg, (0,out_len-targ_len,0,0), value=pad_idx)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[:,1:,:]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        \n",
    "        trg = trg[:,1:]\n",
    "        trg = trg.contiguous().view(-1)\n",
    "\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src)\n",
    "\n",
    "            pad_idx=1\n",
    "            targ_len = trg.size()[1]\n",
    "\n",
    "            out_len = output.size()[1]\n",
    "\n",
    "            if targ_len>out_len:\n",
    "                output  = nn.functional.pad(output,  (0,0,0,targ_len-out_len,0,0), value=pad_idx)\n",
    "            if out_len>targ_len:\n",
    "                trg = nn.functional.pad(trg, (0,out_len-targ_len,0,0), value=pad_idx)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[:,1:,:].contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 20s\n",
      "\tTrain Loss: 5.090 | Train PPL: 162.456\n",
      "\t Val. Loss: 4.562 |  Val. PPL:  95.778\n",
      "Epoch: 02 | Time: 0m 20s\n",
      "\tTrain Loss: 4.491 | Train PPL:  89.172\n",
      "\t Val. Loss: 4.070 |  Val. PPL:  58.533\n",
      "Epoch: 03 | Time: 0m 20s\n",
      "\tTrain Loss: 4.134 | Train PPL:  62.450\n",
      "\t Val. Loss: 3.774 |  Val. PPL:  43.557\n",
      "Epoch: 04 | Time: 0m 20s\n",
      "\tTrain Loss: 3.884 | Train PPL:  48.597\n",
      "\t Val. Loss: 3.628 |  Val. PPL:  37.635\n",
      "Epoch: 05 | Time: 0m 20s\n",
      "\tTrain Loss: 3.698 | Train PPL:  40.382\n",
      "\t Val. Loss: 3.507 |  Val. PPL:  33.339\n",
      "Epoch: 06 | Time: 0m 20s\n",
      "\tTrain Loss: 3.541 | Train PPL:  34.486\n",
      "\t Val. Loss: 3.415 |  Val. PPL:  30.416\n",
      "Epoch: 07 | Time: 0m 20s\n",
      "\tTrain Loss: 3.404 | Train PPL:  30.085\n",
      "\t Val. Loss: 3.348 |  Val. PPL:  28.452\n",
      "Epoch: 08 | Time: 0m 21s\n",
      "\tTrain Loss: 3.281 | Train PPL:  26.614\n",
      "\t Val. Loss: 3.308 |  Val. PPL:  27.321\n",
      "Epoch: 09 | Time: 0m 20s\n",
      "\tTrain Loss: 3.169 | Train PPL:  23.795\n",
      "\t Val. Loss: 3.272 |  Val. PPL:  26.358\n",
      "Epoch: 10 | Time: 0m 20s\n",
      "\tTrain Loss: 3.073 | Train PPL:  21.597\n",
      "\t Val. Loss: 3.251 |  Val. PPL:  25.821\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
